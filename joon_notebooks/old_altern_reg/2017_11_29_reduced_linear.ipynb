{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import h5py\n",
    "import matplotlib.pyplot as plt\n",
    "import math\n",
    "import sys\n",
    "import time\n",
    "from tqdm import tnrange, tqdm_notebook\n",
    "import matplotlib.pyplot as plt\n",
    "import operator\n",
    "from matplotlib.backends.backend_pdf import PdfPages\n",
    "import scipy.io\n",
    "from matplotlib.gridspec import GridSpec\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_reduced_neural_matrix(spike_train_file, image_times_file, save_dir):\n",
    "    train_times = np.asarray(h5py.File(image_times_file, 'r')[\"image_times\"])[0,:]\n",
    "    train_images = train_times.shape[0]\n",
    "    train_times = np.append(train_times, 9999)\n",
    "    spike_train = np.load(spike_train_file)\n",
    "    \n",
    "    spike_times = spike_train[:,0] / 20000\n",
    "    units = np.max(spike_train[:,1]).astype(np.int) + 1\n",
    "    \n",
    "    interval_add = np.arange(0.01, 0.5, 0.01)\n",
    "    \n",
    "    train_matrix = np.zeros((train_images, units, 50))\n",
    "    bin_edges = np.copy(train_times)\n",
    "    \n",
    "    interval_add = np.arange(0.01, 0.5, 0.01)\n",
    "    \n",
    "    for i in tnrange(int(train_images)):\n",
    "        for j in range(interval_add.shape[0]):\n",
    "            bin_edge = train_times[i] + interval_add[j]\n",
    "            bin_edges = np.append(bin_edges, bin_edge)\n",
    "\n",
    "    bin_edges = np.sort(bin_edges)\n",
    "    print(bin_edges.shape)\n",
    "    \n",
    "    hist_spikes, hist_edges = np.histogram(spike_times, bin_edges)\n",
    "    \n",
    "    count = spike_times.shape[0] - np.sum(hist_spikes)\n",
    "    \n",
    "    for i in tnrange(hist_spikes.shape[0]):\n",
    "        bin_count = hist_spikes[i]\n",
    "        for j in range(bin_count):  \n",
    "            image = i//50\n",
    "            unit = int(spike_train[count,1])\n",
    "            index = i - 50*image\n",
    "            train_matrix[image, unit, index] += 1\n",
    "            count += 1\n",
    "        \n",
    "    print(count)\n",
    "    print(spike_train.shape)\n",
    "    \n",
    "    #train_matrix = np.concatenate((train_matrix, np.ones((train_images, units, 1))), axis=2)\n",
    "    \n",
    "    test_matrix = train_matrix[9900:,:,:]\n",
    "    valid_matrix = train_matrix[9800:9900, :,:]\n",
    "    train_matrix = train_matrix[:9800,:,:]\n",
    "    \n",
    "    print(train_matrix.shape)\n",
    "    print(test_matrix.shape)\n",
    "    print(valid_matrix.shape)\n",
    "    print(np.sum(train_matrix) + np.sum(test_matrix) + np.sum(valid_matrix))\n",
    "    \n",
    "    np.save(os.path.join(save_dir, \"yass_50_train_neural.npy\"), train_matrix)\n",
    "    np.save(os.path.join(save_dir, \"yass_50_test_neural.npy\"), test_matrix)\n",
    "    np.save(os.path.join(save_dir, \"yass_50_valid_neural.npy\"), valid_matrix)\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "Unable to allocate array with shape (269571386,) and data type int32",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-4-0508eecf1432>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0msave_dir\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"/ssd/joon/2017_11_29_ns/yass/neural/\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mmake_reduced_neural_matrix\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspike_train_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mimage_times_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msave_dir\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-3-9b13db0c6d33>\u001b[0m in \u001b[0;36mmake_reduced_neural_matrix\u001b[0;34m(spike_train_file, image_times_file, save_dir)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0mtrain_images\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtrain_times\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0mtrain_times\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_times\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9999\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m     \u001b[0mspike_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mspike_train_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m     \u001b[0mspike_times\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mspike_train\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m/\u001b[0m \u001b[0;36m20000\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/npyio.py\u001b[0m in \u001b[0;36mload\u001b[0;34m(file, mmap_mode, allow_pickle, fix_imports, encoding)\u001b[0m\n\u001b[1;32m    451\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    452\u001b[0m                 return format.read_array(fid, allow_pickle=allow_pickle,\n\u001b[0;32m--> 453\u001b[0;31m                                          pickle_kwargs=pickle_kwargs)\n\u001b[0m\u001b[1;32m    454\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    455\u001b[0m             \u001b[0;31m# Try a pickle\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/anaconda3/lib/python3.7/site-packages/numpy/lib/format.py\u001b[0m in \u001b[0;36mread_array\u001b[0;34m(fp, allow_pickle, pickle_kwargs)\u001b[0m\n\u001b[1;32m    736\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0misfileobj\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    737\u001b[0m             \u001b[0;31m# We can use the fast fromfile() function.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 738\u001b[0;31m             \u001b[0marray\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnumpy\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfromfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfp\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcount\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcount\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    739\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    740\u001b[0m             \u001b[0;31m# This is not a real file. We have to read it the\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: Unable to allocate array with shape (269571386,) and data type int32"
     ]
    }
   ],
   "source": [
    "spike_train_file = \"/ssd/joon/2017_11_29_ns/yass/yass_spiketrain.npy\"\n",
    "image_times_file = \"/ssd/joon/2017_11_29_ns/trigger_times_201711290.mat\"\n",
    "save_dir = \"/ssd/joon/2017_11_29_ns/yass/neural/\"\n",
    "\n",
    "make_reduced_neural_matrix(spike_train_file, image_times_file, save_dir)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Initialize V = (k x 50), W = (p x nk)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def initialize_V_W(init_weights_file, k_dim, save_dir):\n",
    "    init_weights = np.load(init_weights_file) #### ((2n+1) x p)\n",
    "    unit_no = init_weights.shape[0]//2\n",
    "    \n",
    "    init_W = np.random.randn(95*146, unit_no * k_dim)\n",
    "    \n",
    "    for i in tnrange(unit_no):\n",
    "        init_W[:, k_dim*i: k_dim*i+2] = init_weights[2*i : 2*i+2, :].T\n",
    "    \n",
    "    print(np.sum(init_weights))\n",
    "    print(np.sum(init_W))\n",
    "    \n",
    "    init_V = np.random.randn(k_dim, 50)\n",
    "    \n",
    "    init_V[0,:] = 0\n",
    "    init_V[0,3:16] = 1\n",
    "    init_V[1,:] = 0\n",
    "    init_V[1,17:31] = 1\n",
    "    \n",
    "    np.save(os.path.join(save_dir, \"yass_init_V.npy\"), init_V)\n",
    "    np.save(os.path.join(save_dir, \"yass_init_W.npy\"), init_W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "003f152d3832454c9a7413b45569ef39",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=2094), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "926567.5358418195\n",
      "928800.5915259448\n"
     ]
    }
   ],
   "source": [
    "init_weights_file = \"/ssd/joon/2017_11_29_ns/yass/weights/yass_smooth_4_weights.npy\"\n",
    "save_dir = \"/ssd/joon/2017_11_29_ns/yass/reduced_rank/\"\n",
    "k_dim = 10\n",
    "\n",
    "initialize_V_W(init_weights_file, k_dim, save_dir)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Iterative Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def V_W_iter_train(I, S, V, k_dim, unit_no, image_no, iteration):\n",
    "    start_time = time.time()        \n",
    "    ####### TRAIN W_NEW #####\n",
    "    R = np.matmul(V, S)\n",
    "    R = R.reshape((k_dim * unit_no, image_no))\n",
    "        \n",
    "    #IRT = np.matmul(I, R.T)\n",
    "    #RRT = np.matmul(R, R.T)\n",
    "    #RRT_inv = np.linalg.inv(RRT + 0.00001*np.identity(RRT.shape[0]))   \n",
    "    #W_new = np.matmul(IRT, RRT_inv)\n",
    "    \n",
    "    W_new = np.matmul(I, np.linalg.pinv(R))\n",
    "        \n",
    "    ####### TRAIN V_NEW ######\n",
    "    W = W_new\n",
    "        \n",
    "    #WTW = np.matmul(W.T, W)\n",
    "    #WTW_inv = np.linalg.inv(WTW + 0.00001 * np.identity(WTW.shape[0]))\n",
    "    #WTI = np.matmul(W.T, I)\n",
    "    #R = np.matmul(WTW_inv, WTI)\n",
    "    \n",
    "    R = np.matmul(np.linalg.pinv(W), I)\n",
    "    R = R.reshape((k_dim, unit_no * image_no))\n",
    "        \n",
    "    #RST = np.matmul(R, S.T)\n",
    "    #SST = np.matmul(S, S.T)\n",
    "    #SST_inv = np.linalg.inv(SST + 0.00001 * np.identity(SST.shape[0]))    \n",
    "    #V_new = np.matmul(RST, SST_inv)\n",
    "    \n",
    "    V_new = np.matmul(R, np.linalg.pinv(S))\n",
    "    V_new = normalize(V_new, axis=1, norm='l1')\n",
    "        \n",
    "    end_time = time.time()\n",
    "        \n",
    "    print(end_time - start_time)\n",
    "        \n",
    "    np.save(os.path.join(save_dir, \"yass_\" +str(iteration)+\"_W.npy\"), W_new)\n",
    "    np.save(os.path.join(save_dir, \"yass_\" +str(iteration)+\"_V.npy\"), V_new)\n",
    "\n",
    "def V_W_train(images_file, neural_file, init_V_file, iter_no, save_dir):\n",
    "    \n",
    "    images = np.load(images_file).T # (95 * 146, images)\n",
    "    neural = np.load(neural_file) # (images, units, 50)\n",
    "    init_V = np.load(init_V_file) # (k , 50)\n",
    "    \n",
    "    image_no = images.shape[1]\n",
    "    unit_no = neural.shape[1]\n",
    "    k_dim = init_V.shape[0]\n",
    "    \n",
    "    I = images - np.mean(images, axis = 1).reshape((images.shape[0], 1))\n",
    "    S = neural - np.mean(neural, axis = 0).reshape((1, neural.shape[1], neural.shape[2]))\n",
    "    S = S.reshape((50, image_no * unit_no))\n",
    "    \n",
    "    #V_array = np.empty((iter_no, k_dim, 50))\n",
    "    #W_array = np.empty((iter_no, 95*146, unit_no * k_dim))\n",
    "    \n",
    "    for i in tnrange(iter_no):\n",
    "        if i == 0:\n",
    "            V_new, W_new = V_W_iter_train(I, S, init_V, k_dim, unit_no, image_no, i)\n",
    "        else:\n",
    "            V_new, W_new = V_W_iter_train(I, S, V_new, k_dim, unit_no, image_no, i)\n",
    "        #V_array[i] = V_new\n",
    "        #W_array[i] = W_new\n",
    "        \n",
    "    #np.save(os.path.join(save_dir, \"yass_V_iterations.npy\"), V_array)\n",
    "    #np.save(os.path.join(save_dir, \"yass_W_iterations.npy\"), W_array)   \n",
    "        \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "be4c13f7ebb94cc0b14182c24c24bf74",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(IntProgress(value=0, max=1), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "images_file = \"/ssd/joon/2017_11_29_ns/images/smooth_train_images.npy\"\n",
    "neural_file = \"/ssd/joon/2017_11_29_ns/yass/neural/yass_50_train_neural.npy\"\n",
    "init_V_file = \"/ssd/joon/2017_11_29_ns/yass/reduced_rank/yass_init_V.npy\"\n",
    "save_dir = \"/ssd/joon/2017_11_29_ns/yass/reduced_rank/\"\n",
    "iter_no = 1\n",
    "\n",
    "V_W_train(images_file, neural_file, init_V_file, iter_no, save_dir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "images_file = \"/ssd/joon/2017_11_29_ns/images/smooth_train_images.npy\"\n",
    "neural_file = \"/ssd/joon/2017_11_29_ns/yass/neural/yass_50_train_neural.npy\"\n",
    "init_V_file = \"/ssd/joon/2017_11_29_ns/yass/reduced_rank/yass_init_V.npy\"\n",
    "save_dir = \"/ssd/joon/2017_11_29_ns/yass/reduced_rank/\"\n",
    "iter_no = 1\n",
    "\n",
    "V_W_train(images_file, neural_file, init_V_file, iter_no, save_dir)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
